{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "##### Using external dataset to get similar words\n",
    "Created by Nontas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U gensim\n",
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/berry/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /Users/berry/nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import spacy\n",
    "import os \n",
    "import collections\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "#nltk.download()\n",
    "from nltk.corpus import brown\n",
    "nltk.download('word2vec_sample')\n",
    "from nltk.data import find\n",
    "import gensim #import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"../data/Cleaned_DS_Jobs.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['job_desc'] = data['Job Description'].str.lower()\n",
    "data['job_desc'] = data['Job Description'].str.replace('\\\\n',' ')\n",
    "data['job_desc'] = data['Job Description'].str.replace('http\\S+|www\\S+|https\\S+','')\n",
    "data['job_desc'] = data['Job Description'].str.replace('\\S+@\\S+\\.\\S+','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to remove unwanted in job description, kept digits \n",
    "stop_words = stopwords.words('english')\n",
    "to_remove2 = stop_words + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['job_desc_token'] = data['job_desc'].apply(word_tokenize).apply(lambda x: [item for item in x if item not in to_remove2])\n",
    "data['job_desc_clean'] = data['job_desc_token'].apply(TreebankWordDetokenizer().detokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec\n",
    "Build model from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_train = data.job_desc_token.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = Word2Vec(sent_train, sg = 1) #sg :Training algorithm: 1 for skip-gram; otherwise CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column of bigram tokens for custom lists \n",
    "data['job_desc_bigrams'] = data['job_desc_clean'].apply(lambda row: list(nltk.ngrams(row.split(\" \"), 2)))\n",
    "bi_list = data.job_desc_bigrams.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.wv.most_similar(['experience'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('brown.embedding')\n",
    "brown_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_model.wv.most_similar(['experience'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-trained NLTK model (Google News)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reset all variables in jupyter notebook\n",
    "#%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.KeyedVectors object at 0x7fc3b1dd40a0>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('experiences', 0.6655041575431824), ('expertise', 0.6086293458938599), ('knowledge', 0.5741490125656128), ('experienced', 0.529401957988739), ('familiarity', 0.5093621611595154), ('skills', 0.4901728332042694), ('Experience', 0.4896180033683777), ('perspective', 0.46464166045188904), ('background', 0.46263056993484497), ('acumen', 0.4565761685371399)]\n"
     ]
    }
   ],
   "source": [
    "output = model.most_similar(positive=['experience'], topn = 10)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Suggestion  Percentage similar\n",
      "0  experiences            0.665504\n",
      "1    expertise            0.608629\n",
      "2    knowledge            0.574149\n",
      "3  experienced            0.529402\n",
      "4  familiarity            0.509362\n",
      "5       skills            0.490173\n",
      "6   Experience            0.489618\n",
      "7  perspective            0.464642\n",
      "8   background            0.462631\n",
      "9       acumen            0.456576\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(output, columns=['Suggestion','Percentage similar'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fba33e78b77f025177f80ea29759b55d5960913694c1540010dc608131b64ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
